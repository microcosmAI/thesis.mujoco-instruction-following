{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from MuJoCo_Gym.mujoco_rl import MuJoCo_RL\n",
    "from MuJoCo_Gym.single_agent_wrapper import Single_Agent_Wrapper\n",
    "import time\n",
    "from stable_baselines3 import PPO, SAC\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a reward function\n",
    "In version 3.0, the reward function is not a class, but an actual callable function, which is also handed over as an argument in the config dictionary. First the data fields are created in the dataStore. This will be done every time the environment has been reset, as the datastore is cleared back to {agent:{}, agent2:{} etc.} during reset.<br>\n",
    "After that the agent gets a reward for getting closer to the target. To achieve this the reward function simply calculates the difference between the current distance and the distance at the previous timestep. If the agent gets closer to the target, the difference is positive and therefor the reward is positive as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(mujoco_gym, agent):\n",
    "    # Creates all the necessary fields to store the needed data within the dataStore at timestep 0\n",
    "    if \"targets\" not in mujoco_gym.dataStore.keys():\n",
    "        mujoco_gym.dataStore[\"targets\"] = mujoco_gym.filterByTag(\"target\")\n",
    "        mujoco_gym.dataStore[agent][\"current_target\"] = mujoco_gym.dataStore[\"targets\"][random.randint(0, len(mujoco_gym.dataStore[\"targets\"]) - 1)][\"name\"]\n",
    "        distance = mujoco_gym.distance(agent, mujoco_gym.dataStore[agent][\"current_target\"])\n",
    "        mujoco_gym.dataStore[agent][\"distance\"] = distance\n",
    "        new_reward = 0\n",
    "    else: # Calculates the distance between the agent and the current target\n",
    "        distance = mujoco_gym.distance(agent, mujoco_gym.dataStore[agent][\"current_target\"])\n",
    "        new_reward = mujoco_gym.dataStore[agent][\"distance\"] - distance\n",
    "        mujoco_gym.dataStore[agent][\"distance\"] = copy.deepcopy(distance)\n",
    "    reward = new_reward * 10\n",
    "    return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done function\n",
    "The current simulation run is over if the agent gets closer than one distance unit to the target. Note that the data field for distance does not have to be created again. This is because the reward functions are executed before the done function inside the environment class. This means that the distance fields already exist, even at timestep 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done_function(mujoco_gym, agent):\n",
    "    if mujoco_gym.dataStore[agent][\"distance\"] <= 1 or mujoco_gym.dataStore[agent][\"distance\"] > 15:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the environment\n",
    "The path to the mujoco xml file and additional json info file are handed over, as well as the agents mujoco names and the reward/done function and the environment dynamic. The render mode is also set to true, meaning that the environment is rendered on screen while running. This should only be done for inference though, as rendering is quite ressource intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/Ray/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "environment_path = \"Environment/MultiEnvs.xml\"\n",
    "info_path = \"Environment/info_example.json\"\n",
    "agents = [\"agent1_torso\"]\n",
    "config_dict = {\"xmlPath\":environment_path, \"infoJson\":info_path, \"agents\":agents, \"rewardFunctions\":[reward_function], \"doneFunctions\":[done_function], \"renderMode\":True}\n",
    "environment = MuJoCo_RL(config_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the environment into a single agent openAI gym\n",
    "To make the environment compatible with Stable Baselines3, it has to be converted into an OpenAI Gym first. This can be achieved by using the Single_Agent_Wrapper class, which implements a gym interface for the MultiAgentEnv. Note that the multi agent environment can only have one active agent. If more than one are handed over, the environment will throw an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymEnvironment = Single_Agent_Wrapper(environment, agents[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent using Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04659a073d33440f8e659f426eeffe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+03 |\n",
      "|    ep_rew_mean     | 3.26     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 2376     |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 4100     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.02e+03 |\n",
      "|    ep_rew_mean     | 3.37     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 2526     |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8200     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m policy_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(net_arch\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(pi\u001b[39m=\u001b[39m[\u001b[39m4096\u001b[39m, \u001b[39m4096\u001b[39m, \u001b[39m4096\u001b[39m], qf\u001b[39m=\u001b[39m[\u001b[39m4096\u001b[39m, \u001b[39m4096\u001b[39m, \u001b[39m4096\u001b[39m]))\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SAC(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, gymEnvironment, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, train_freq\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m\"\u001b[39m), batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, learning_starts\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0015\u001b[39m, buffer_size\u001b[39m=\u001b[39m\u001b[39m1500000\u001b[39m, policy_kwargs\u001b[39m=\u001b[39mpolicy_kwargs, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m5000000\u001b[39;49m, progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/sac/sac.py:302\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[1;32m    295\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    301\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    303\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    304\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    305\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    307\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    308\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:540\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mreset_noise(env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    539\u001b[0m \u001b[39m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39;49mnum_envs)\n\u001b[1;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:372\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    367\u001b[0m     unscaled_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_envs)])\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     \u001b[39m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[39m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[39m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     unscaled_action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_last_obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    374\u001b[0m \u001b[39m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:539\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    526\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/miniconda3/envs/Ray/lib/python3.9/site-packages/stable_baselines3/common/policies.py:348\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    346\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(observation, deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m    347\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msquash_output:\n\u001b[1;32m    352\u001b[0m         \u001b[39m# Rescale to proper domain when using squashing\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[4096, 4096, 4096], qf=[4096, 4096, 4096]))\n",
    "model = SAC(\"MlpPolicy\", gymEnvironment, verbose=1, train_freq=(1, \"episode\"), batch_size=256, learning_starts=10000, learning_rate=0.0001, buffer_size=1500000, policy_kwargs=policy_kwargs, device=\"mps\")\n",
    "model.learn(total_timesteps=5000000, progress_bar=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/sac_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference while rendering the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(\"models/sac_model\")\n",
    "testEnv = MuJoCo_RL(config_dict)\n",
    "testEnvSingleAgent = Single_Agent_Wrapper(testEnv, agents[0])\n",
    "obs = testEnvSingleAgent.reset()\n",
    "reward = 0\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = testEnvSingleAgent.step(action)\n",
    "    if dones:\n",
    "        print(reward)\n",
    "        break\n",
    "    reward += rewards\n",
    "    time.sleep(0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
