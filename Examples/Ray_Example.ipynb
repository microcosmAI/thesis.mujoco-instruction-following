{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MuJoCo_Gym.mujoco_rl import MuJoCo_RL\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a language channel\n",
    "This language channel sends one token per timestep two the other agent. Which token is determined by which number the agents picks between 0 and 2. As observations the agents receives the token the other agent uttered one timestep earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.observation_space = {\"low\": [0, 0, 0], \"high\": [1, 1, 1]}\n",
    "        self.action_space = {\"low\": [0, 0, 0], \"high\": [1, 1, 1]}\n",
    "        self.dataStore = {}\n",
    "\n",
    "    def dynamic(self, agent, actions):\n",
    "        if \"utterance\" not in self.environment.dataStore[agent].keys():\n",
    "            self.environment.dataStore[agent][\"utterance\"] = 0\n",
    "\n",
    "        utterance = [0, 0, 0]\n",
    "        utterance[np.argmax(actions)] = 1\n",
    "        self.environment.dataStore[agent][\"utterance\"] = utterance\n",
    "\n",
    "        otherAgent = [other for other in self.environment.agents if other != agent][0]\n",
    "\n",
    "        if \"utterance\" in self.environment.dataStore[otherAgent]:\n",
    "            utteranceOtherAgent = self.environment.dataStore[otherAgent][\"utterance\"]\n",
    "            return 0, np.array(utteranceOtherAgent)\n",
    "        else:\n",
    "            return 0, np.array([0, 0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a reward function\n",
    "In version 3.0, the reward function is not a class, but an actual callable function, which is also handed over as an argument in the config dictionary. First the data fields are created in the dataStore. This will be done every time the environment has been reset, as the datastore is cleared back to {agent:{}, agent2:{} etc.} during reset.<br>\n",
    "After that the agent gets a reward for getting closer to the target. To achieve this the reward function simply calculates the difference between the current distance and the distance at the previous timestep. If the agent gets closer to the target, the difference is positive and therefor the reward is positive as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(mujoco_gym, agent):\n",
    "    # Creates all the necessary fields to store the needed data within the dataStore at timestep 0 \n",
    "    if \"targets\" not in mujoco_gym.dataStore[agent].keys():\n",
    "        mujoco_gym.dataStore[\"targets\"] = mujoco_gym.filterByTag(\"target\")\n",
    "        mujoco_gym.dataStore[agent][\"current_target\"] = mujoco_gym.dataStore[\"targets\"][random.randint(0, len(mujoco_gym.dataStore[\"targets\"]) - 1)][\"name\"]\n",
    "        distance = mujoco_gym.distance(agent, mujoco_gym.dataStore[agent][\"current_target\"])\n",
    "        mujoco_gym.dataStore[agent][\"distance\"] = distance\n",
    "        new_reward = 0\n",
    "    else: # Calculates the distance between the agent and the current target\n",
    "        distance = mujoco_gym.distance(agent, mujoco_gym.dataStore[agent][\"current_target\"])\n",
    "        new_reward = mujoco_gym.dataStore[agent][\"distance\"] - distance\n",
    "        mujoco_gym.dataStore[agent][\"distance\"] = distance\n",
    "    reward = new_reward * 10\n",
    "    return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done function\n",
    "The current simulation run is over if the agent gets closer than one distance unit to the target. Note that the data field for distance does not have to be created again. This is because the reward functions are executed before the done function inside the environment class. This means that the distance fields already exist, even at timestep 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done_function(mujoco_gym, agent):\n",
    "    if mujoco_gym.dataStore[agent][\"distance\"] <= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the environment\n",
    "The path to the mujoco xml file and additional json info file are handed over, as well as the agents mujoco names and the reward/done function and the environment dynamic. The render mode is also set to true, meaning that the environment is rendered on screen while running. This should only be done for inference though, as rendering is quite ressource intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/Ray/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "environment_path = \"Environment/SingleBoxEnv.xml\"\n",
    "info_path = \"Environment/info_example.json\"\n",
    "agents = [\"agent1_torso\", \"agent2_torso\"]\n",
    "config_dict = {\"xmlPath\":environment_path, \"infoJson\":info_path, \"agents\":agents, \"rewardFunctions\":[reward_function], \"doneFunctions\":[done_function], \"environmentDynamics\":[Language], \"freeJoint\":True, \"renderMode\":True, \"maxSteps\":4096}\n",
    "environment = MuJoCo_RL(config_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulation\n",
    "The simulation is run with sample data from the action spaces of the individual agents. They are being reset if either truncations or terminations return true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, 0.0, 1.0, 0.0] [0 0 0]\n",
      "[1.4000000000000001, 0.0, 1.0, 0.0] [1 0 0]\n",
      "[-1.0, 0.0, 1.0, 0.0] [1 0 0]\n",
      "[1.4000000000000001, 0.0, 1.0, 0.0] [1 0 0]\n",
      "[-1.0, 0.0, 1.0, 0.0] [0 0 1]\n",
      "[1.4000000000000001, 0.0, 1.0, 0.0] [1 0 0]\n",
      "[-1.0, 0.0, 1.0, 0.0] [0 1 0]\n",
      "[1.4000000000000001, 0.0, 1.0, 0.0] [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "while True:\n",
    "    try:\n",
    "        # print(test_env._action_space)\n",
    "        action = {\"agent1_torso\": environment._action_space[\"agent1_torso\"].sample(), \"agent2_torso\": environment._action_space[\"agent2_torso\"].sample()}\n",
    "        observations, current_rewards, terminations, truncations, infos = environment.step(action)\n",
    "\n",
    "        if terminations[\"__all__\"] == True or truncations[\"__all__\"] == True:\n",
    "            environment.reset()\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujocoTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
