{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3 import SAC\n",
    "import supersuit as ss\n",
    "from pettingzoo.test import api_test, parallel_api_test\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/cowolff/Documents/GitHub/s.mujoco_environment/Gym\")\n",
    "from single_agent import SingleAgent\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(mujoco_gym, data, model) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the reward based only on the agent's distance to the target\n",
    "    Parameters:\n",
    "        agent (str): name of the agent\n",
    "        target (str): name of the target\n",
    "    Returns:\n",
    "        reward (float): reward for the agent\n",
    "    \"\"\"\n",
    "    distance = math.dist(data.body(\"torso\").xipos, data.body(\"target\").xipos)\n",
    "    if mujoco_gym.lastDistance is None:\n",
    "        mujoco_gym.lastDistance = distance\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = mujoco_gym.lastDistance - distance\n",
    "        mujoco_gym.lastDistance = distance\n",
    "    if mujoco_gym.use_head_sensor:\n",
    "        if mujoco_gym.data.sensordata.flat[4] < 0.15:\n",
    "            reward = reward - 0.01\n",
    "    return reward * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(mujoco_gym, data, model) -> float:\n",
    "    \"\"\"\n",
    "    Implementation of the test reward function.\n",
    "    It contains two parts:\n",
    "    1. The agent gets a reward for moving towards the target\n",
    "    2. The agent gets a reward for moving at all\n",
    "    Both rewards are equally weighted.\n",
    "    Parameters:\n",
    "        agent (str): name of the agent\n",
    "        target (str): name of the target\n",
    "    Returns:\n",
    "        reward (float): reward for the agent\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    distance = math.dist(data.body(\"torso\").xipos, data.body(mujoco_gym.data_store[\"current_target\"]).xipos)\n",
    "    if \"distance\" not in mujoco_gym.data_store.keys():\n",
    "        mujoco_gym.data_store[\"distance\"] = distance\n",
    "        new_reward = 0\n",
    "    else:\n",
    "        new_reward = mujoco_gym.data_store[\"distance\"] - distance\n",
    "        mujoco_gym.data_store[\"distance\"] = distance\n",
    "        new_reward * 10\n",
    "    reward = reward + new_reward\n",
    "\n",
    "    if \"last_position\" not in mujoco_gym.data_store.keys():\n",
    "        mujoco_gym.data_store[\"last_position\"] = copy.deepcopy(data.body(\"torso\").xipos)\n",
    "        new_reward = 0\n",
    "    else:\n",
    "        new_reward = math.dist(mujoco_gym.data_store[\"last_position\"], data.body(\"torso\").xipos)\n",
    "        mujoco_gym.data_store[\"last_position\"] = copy.deepcopy(data.body(\"torso\").xipos)\n",
    "        new_reward = new_reward * 10\n",
    "        if new_reward < 0.8:\n",
    "            new_reward = new_reward * -1\n",
    "    reward = reward + (new_reward * 0.6)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_up_dynamic(mujoco_gym, data, model):\n",
    "    \"\"\"\n",
    "    Update Target position and adds the inventory to the agent as an observation\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if \"inventory\" not in mujoco_gym.data_store.keys():\n",
    "        mujoco_gym.data_store[\"inventory\"] = []\n",
    "        mujoco_gym.data_store[\"inventory\"].append(0)\n",
    "    if \"targets\" not in mujoco_gym.data_store.keys():\n",
    "        mujoco_gym.data_store[\"targets\"] = mujoco_gym.filterByTag(\"target\")\n",
    "        mujoco_gym.data_store[\"current_target\"] = mujoco_gym.data_store[\"targets\"][random.randint(0, len(mujoco_gym.data_store[\"targets\"]) - 1)][\"name\"]\n",
    "    distance = mujoco_gym.calculate_distance(\"torso\", mujoco_gym.data_store[\"current_target\"])\n",
    "    if distance < 2:\n",
    "        print(\"target reached\")\n",
    "        reward = 0\n",
    "        if mujoco_gym.data_store[\"inventory\"][0] == 0:\n",
    "            mujoco_gym.data_store[\"inventory\"][0] = 1\n",
    "            reward = 1\n",
    "        elif mujoco_gym.data_store[\"inventory\"][0] == 1:\n",
    "            mujoco_gym.data_store[\"inventory\"][0] = 0\n",
    "            reward = 1\n",
    "        mujoco_gym.data_store[\"current_target\"] = mujoco_gym.data_store[\"targets\"][random.randint(0, len(mujoco_gym.data_store[\"targets\"]) - 1)][\"name\"]\n",
    "        mujoco_gym.data_store[\"distance\"] = math.dist(data.body(\"torso\").xipos, data.body(mujoco_gym.data_store[\"current_target\"]).xipos)\n",
    "    return reward, np.concatenate((data.body(mujoco_gym.data_store[\"current_target\"]).xipos, mujoco_gym.data_store[\"inventory\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = SingleAgent(\"/Users/cowolff/Documents/GitHub/s.mujoco_environment/Environments/single_agent/ModelVis.xml\", info_json=\"/Users/cowolff/Documents/GitHub/s.mujoco_environment/Environments/single_agent/info_example.json\", render=False, print_camera_config=False, add_target_coordinates=False, add_agent_coordinates=True, end_epoch_on_turn=True, env_dynamics=[pick_up_dynamic], reward_function=test_reward, max_step=8192, use_ctrl_cost=False)\n",
    "    print(\"env created\")\n",
    "    obs = env.reset()\n",
    "    print(env.filterByTag(\"target\"))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = SingleAgent(\"/Users/cowolff/Documents/GitHub/s.mujoco_environment/Environments/single_agent/ModelVis.xml\", info_json=\"/Users/cowolff/Documents/GitHub/s.mujoco_environment/Environments/single_agent/info_example.json\", render=False, print_camera_config=False, add_target_coordinates=False, add_agent_coordinates=True, end_epoch_on_turn=True, env_dynamics=[pick_up_dynamic], reward_function=test_reward, max_step=8192, use_ctrl_cost=False)\n",
    "    print(\"env created\")\n",
    "    # layer = dict(activation_fn=th.nn.ReLU, net_arch=[dict(pi=[1024, 512, 256], vf=[1024, 512, 256])])\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[4096, 2048, 1024], qf=[4096, 2048, 1024]))\n",
    "    model = SAC(\"MlpPolicy\", env, verbose=1, train_freq=(128, \"step\"), batch_size=128, learning_starts=100000, learning_rate=0.0015, buffer_size=1500000, policy_kwargs=policy_kwargs)\n",
    "    print(\"model created\")\n",
    "    model.learn(total_timesteps=2000000, progress_bar=True)\n",
    "    print(\"model trained\")\n",
    "    model.save(\"models/sac_model\")\n",
    "    env2 = MyEnv(\"envs/ModelVis.xml\", render=True, print_camera_config=False, add_target_coordinates=True, add_agent_coordinates=True, end_epoch_on_turn=True, max_step=8192)\n",
    "    obs = env2.reset()\n",
    "    for i in range(8192):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env2.step(action, False)\n",
    "        if dones:\n",
    "            print(rewards)\n",
    "            obs = env2.reset()\n",
    "        time.sleep(0.1)\n",
    "        env2.render()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer():\n",
    "    model = SAC.load(\"models/sac_model\")\n",
    "    env = SingleAgent(\"envs/ModelVis.xml\", info_json=\"envs/info_example.json\", render=True, print_camera_config=False, add_target_coordinates=False, add_agent_coordinates=True, end_epoch_on_turn=True, env_dynamics=[pick_up_dynamic], reward_function=test_reward, max_step=8192, use_ctrl_cost=False)\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    for i in range(512):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action, False)\n",
    "        if dones:\n",
    "            print(reward)\n",
    "            break\n",
    "        reward += rewards\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "    env.reset()\n",
    "    env.end()\n",
    "infer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
