{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Function and Environment Dynamic Class(es) for Single Agent Gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single agent gym (SingleAgent()), named mujoco_gym in the following, enables the training of a single agent with reinforcement learning. To instaniate a mujoco_gym, among other parameters 1. the reward function and 2. the environment dynamic class(es) need to be specified. How these may be implemented is explained in the following."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reward Function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reward function determines the reward an agent receives after an action in a gym. In our test_reward function an agent receives a reward for both 1.1 moving towards a target and 1.2 moving at all in the mujoco_gym. The final reward returned by the function is the sum of these two. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Reward for moving towards a target\n",
    "\n",
    "The variable distance (line 1) stores the distance between the \"torso\" and the \"current_target\". For that it uses the mujoco_gym function calculate_distance, which can handle arrays with the coordinates of an object as well as strings with the name of an object to read out this objects coordinates. \n",
    "Next, the if-else query (lines 2-7) checks whether the dictionary data_store from the mujoco_gym already has the key \"distance\". \n",
    "If not (lines 2-4) the entry \"distance\" is created with the value of the variable distance. The (new_)reward is zero in this case, as the agent never attempted to move towards the \"current_target\" before. \n",
    "Otherwise (lines 5-7), if \"distance\" already exists in the data_store, it represents the distance to the \"current_target\" before the last step. The difference between this former distance and the current distance is the (new_)reward. If the \"torso\" moved closer to the \"current_target\" it is positive. If the agent did not move at all it is zero, and if the agent moved further away, it is negative. Then the data_store dictionary entry \"distance\" is updated with the current distance.\n",
    "Lastly, the variable reward (line 8) stores the new_reward to allow an easy addition later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = mujoco_gym.calculate_distance(\"torso\", mujoco_gym.data_store[\"current_target\"])\n",
    "if \"distance\" not in mujoco_gym.data_store.keys():\n",
    "    mujoco_gym.data_store[\"distance\"] = distance\n",
    "    new_reward = 0\n",
    "else:\n",
    "    new_reward = mujoco_gym.data_store[\"distance\"] - distance\n",
    "    mujoco_gym.data_store[\"distance\"] = distance\n",
    "reward = new_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Reward for moving at all\n",
    "\n",
    "Similarily to 1., the if-else-query (lines 1-9) checks whether the dictionary data_store from the mujoco_gym already has the key \"last_position\". \n",
    "If not (lines 1-3) the entry \"last_position\" is created with a deepcopy of the coordinates (np.ndarray) from the \"torso\". The (new_)reward is zero in this case, as the agent never attempted to move before. \n",
    "Otherwise (lines 4-9), if \"last_position\" already exists in the data_store, the agents position was checked before the last step. The (new_)reward is the distance between the agents last and its current position. It is calculated with the calculate_distance function from mujoco_gym, like the distance between the \"torso\" and the \"current_target\" in 1. Next, the data_store entry \"last_position\" is updated to the current position to be used in the next step. If the movement of the agent was too small, namely less than 0.08 (line 7), the agent receives a negative reward. Reward shaping revealed to multiply this (new_)reward by a factor of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"last_position\" not in mujoco_gym.data_store.keys():\n",
    "    mujoco_gym.data_store[\"last_position\"] = copy.deepcopy(mujoco_gym.data.body(\"torso\").xipos)\n",
    "    new_reward = 0\n",
    "else:\n",
    "    new_reward = mujoco_gym.calculate_distance(\"torso\", mujoco_gym.data_store[\"last_position\"])\n",
    "    mujoco_gym.data_store[\"last_position\"] = copy.deepcopy(mujoco_gym.data.body(\"torso\").xipos)\n",
    "    if new_reward < 0.08:\n",
    "        new_reward = new_reward * -1\n",
    "    new_reward = new_reward * 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the return variable reward is the sum of the reward for moving towards the \"current target\" and the new_reward for moving at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = reward + new_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Environment Dynamic Class(es)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment Dynamics enable updating features of the data store and giving out an additional reward on special occassions. They are written in separate classes to set boundaries for the observation space and thus enable normalizing the observations more precicely than by normalizing with infinity. So far there are two Environment Dynamics: 2.1 Environment Dynamic and 2.2 Pick Up Dynamic. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Environment Dynamic Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Attributes \n",
    "\n",
    "The Environment Dynamics receive a mujoco_gym as parameter. Besides this, it has an observation space. That is, a dicitonary which contains the lowest and highest values of the return variable target_coordinates to normalize it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 dynamic Method\n",
    "The function dynamic updates the target if an agent is close enough. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it first checks with an if query (lines 1-3), whether the data store of an agent already contains targets. If not, it adds the entry \"targets\" to the data_store dictionary with every object in the environment which has the tag \"target\", this excludes e.g. the agent or walls. Afterwards, a random target is selected to be the current target for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"targets\" not in self.mujoco_gym.data_store.keys():\n",
    "    self.mujoco_gym.data_store[\"targets\"] = self.mujoco_gym.filterByTag(\"target\")\n",
    "    self.mujoco_gym.data_store[\"current_target\"] = self.mujoco_gym.data_store[\"targets\"][random.randint(0, len(self.mujoco_gym.data_store[\"targets\"]) - 1)][\"name\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as the agent has a current target, the distance between the agent and its target is calculated. Therefore, we us the same function like in the reward funciton (see above). If the agent is close to the target (lines 2-3), a new target is selected and the distance to the new target is calculated and stored in the data store. The SingleAgent class requires each Environment Dynamic class to return a reward, which is useful in some cases (see 2.2). As this feature is not used here, it is simply set to 0. Lastly, the coordinates of the current target are read out from the data store and together with the 0 reward returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = self.mujoco_gym.calculate_distance(\"torso\", self.mujoco_gym.data_store[\"current_target\"])\n",
    "if distance < 1:\n",
    "    self.mujoco_gym.data_store[\"current_target\"] = self.mujoco_gym.data_store[\"targets\"][random.randint(0, len(self.mujoco_gym.data_store[\"targets\"]) - 1)]\n",
    "    self.mujoco_gym.data_store[\"distance\"] = self.mujoco_gym.calculate_distance(\"torso\", self.mujoco_gym.data_store[\"current_target\"])\n",
    "reward = 0\n",
    "target_coordinates = self.mujoco_gym.data.body(self.mujoco_gym.data_store[\"current_target\"]).xipos\n",
    "return reward, target_coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Pick Up Dynamic Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 Attributes\n",
    "The attributes are quite similar to the Environment Dynamic Class. The only difference lies in the additional entry in the values of the observation space. This entry tells whether an agent carries an object or not. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 dynamic Method \n",
    "The function dynamic updates the target if an agent is close enough as well as adds or removes an inventory to or from the agent as an observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the method checks whether all needed keys are in the data store. Therefore, it checks if \"inventory\" is in the data store (lines 1-2) and adds it with a list containing only a 0 if neccessary. Furthermore, it checks whether the data store has \"targets\" (lines 3-5). Occasionally it sets the values to a list of all objects tagged as target and randomly chooses one of them as current target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"inventory\" not in self.mujoco_gym.data_store.keys():\n",
    "    self.mujoco_gym.data_store[\"inventory\"] = [0]\n",
    "if \"targets\" not in self.mujoco_gym.data_store.keys():\n",
    "    self.mujoco_gym.data_store[\"targets\"] = self.mujoco_gym.filterByTag(\"target\")\n",
    "    self.mujoco_gym.data_store[\"current_target\"] = self.mujoco_gym.data_store[\"targets\"][random.randint(0, len(self.mujoco_gym.data_store[\"targets\"]) - 1)][\"name\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the distance between the agent and the current target is calculated. If the agent is close to the target (lines 1-10), the target is reached and the reward feature of the Environment Dynamic comes into play. An if-elif query, checks whether the agents inventory is containing an object. If not (lines 3-5), the agent picks up the target by incrementing the inventory and receives a reward. Otherwise (lines 6-8), the agent drops its inventory by setting the value to 0 and receives a reward. Thereafter a new target is randomly chosen and the distance to the new target is calculated and stored in the data store. \n",
    "Lastly, the coordinates of the target and the inventory are concatenated to one array and returned together with the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = self.mujoco_gym.calculate_distance(\"torso\", self.mujoco_gym.data_store[\"current_target\"])\n",
    "if distance < 2:\n",
    "    print(\"target reached\")\n",
    "    if self.mujoco_gym.data_store[\"inventory\"][0] == 0:\n",
    "        self.mujoco_gym.data_store[\"inventory\"][0] = 1\n",
    "        reward = 1\n",
    "    elif self.mujoco_gym.data_store[\"inventory\"][0] == 1:\n",
    "        self.mujoco_gym.data_store[\"inventory\"][0] = 0\n",
    "        reward = 1\n",
    "    self.mujoco_gym.data_store[\"current_target\"] = self.mujoco_gym.data_store[\"targets\"][random.randint(0, len(self.mujoco_gym.data_store[\"targets\"]) - 1)][\"name\"]\n",
    "    self.mujoco_gym.data_store[\"distance\"] = self.mujoco_gym.calculate_distance(\"torso\", self.mujoco_gym.data_store[\"current_target\"])\n",
    "current_target_coordinates_with_inventory = np.concatenate((self.mujoco_gym.data.body(self.mujoco_gym.data_store[\"current_target\"]).xipos, self.mujoco_gym.data_store[\"inventory\"]))\n",
    "return reward, current_target_coordinates_with_inventory"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
